\documentclass[12pt]{article}

\title{Tour merging via tree decomposition \\ \vspace{2mm} \small{A hybrid approach between
heuristics and exact solutions for TSP and VRP.}}
\author{Mattias Beimers - 3672565}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{algpseudocode}
\usepackage[margin=1.5in]{geometry}
\usepackage{hyperref}

\newcommand{\ee}[1]{\ensuremath{\times 10^{#1}}}
\newcommand{\ccP}[0]{\textsc{P}}
\newcommand{\ccNP}[0]{\textsc{NP}}


\begin{document}
\tikzstyle{vertex2} = [circle,fill=black!25,minimum size=24pt,align=center,font=\small,inner sep=0pt]
\tikzstyle{vertex1} = [circle,fill=black!75,minimum size=4pt,align=center,inner sep=0pt]
\tikzstyle{vertex0} = [circle,fill=black!75,minimum size=1pt,inner sep=0pt]
\tikzstyle{bag} = [circle,fill=black!25,minimum size=45pt,align=center,text width=40pt,font=\small,inner sep=0pt]
\tikzstyle{edge} = [draw=black!75,-]
\tikzstyle{arc} = [draw,->-]
\tikzstyle{weight} = [font=\small]

\maketitle

% Notes:
% Whenever I mention a graph, I should call it a complete graph, that will be sufficiently precise.
% A complete graph is a simple undirected graph in which every pair of distinct vertices is
% connected by a unique edge.
% A simple graph is an unweighted, undirected graph containing no self loops or multiple edges.
%
% Things to think of:
% - A tour SATISFIES a demand; it doesn't HAVE a demand.
% - Use of the dash (i.e. The Lin-Kernighan-Helsgaun heuristic)
% - Don't say this PAPER, but say this THESIS
% - The word Section with a capital letter
% - Don't use the word treewidth when you mean 'width of the decomposition'.
%
% Plot graphs with MathPlotLib: http://matplotlib.org/ (possibly ask Chiel for example)
%

%
% Abstract
%
\begin{abstract}
    % About TSP and VRP and what this thesis does
    A hybrid approach between heuristics and exact solutions for TSP and VRP using tree
    decompositions.
    % TODO
\end{abstract}



%
% The introduction
%
\section{Introduction}
\label{sec:introduction}

% Heuristics and Cook-Seymour
For many optimization problems calculating provably optimal solutions is not feasible in practical
applications, because the computation time grows exponentionally with the problem size. Hence,
heuristics are used to find solutions that are good, but not nescessarily optimal.
To get more certainty that a solution is good, or to improve the solution even more, the heuristics
are often applied multiple times and the best solution is selected. Although this works well, Cook
and Seymour noted in their work on the Traveling Salesman Problem~\cite{cook-seymour} that by
discarding all but the best solution, possibly valuable information is lost. Hence the idea emerged
to merge all the found solution tours in a single graph and calculate the optimal solution on the
branch-decomposition of that graph.

% TODO: is this really at the time of publishing?????
% (1-2 years before - sept 2001), while publish date is 2002 or (2003 according to google scholar)
% TODO: find reference
                                                                                    % TODO TODO TODO TODO TODO TODO TODO TODO
At the time of publishing, the solutions found by Cook and Seymour improved on the best known
results for instances with almost 25000 vertices~\cite{the sweden instance}. Since then other
heuristics have improved massively and outperform the approach by Cook and Seymour~\cite{lkh2}. In
this thesis, we will try if the strategy for TSP by Cook and Seymour can still improve current
heuristics even more. Furthermore, we will try to extend it to work for the Vehicle Routing Problem
(VRP).

% TSP and VRP; desription and defenition
The Traveling Salesman Problem (TSP) is one of the most well studied NP-hard problems, where a
merchant wants to visit a number of cities and get back at his starting point in the shortest
possible amount of time.
We recognize the TSP problem in many practical applications, from planning a school bus route to
scheduling a machine to drill holes in a circuit board.
% More examples in the LKH 1 paper
A generalized version of this problem, where there are not one but a number of merchants (or trucks)
visiting the cities from the starting point (or depot), is widely used in the transportation sector.
This problem is known as the Vehicle Routing Problem (VRP).

We define the TSP, given a complete graph $G' = (V, E')$, as finding a tour, or cycle, that visits
all cities exactly once with smallest total cost. For this thesis we assume the cost $c_e$ of an
edge $e = (v, w)$ is the euclidean distance between $v$ and $w$.
Given additionally a demand $d_v$ for each vertex $v$, a maximum capacity $C$ of goods per truck, a
number $M$ of available trucks and a special vertex $v_0$ that is the depot, we can define the VRP
as finding a set of at most $M$ tours with the least total cost. Each tour has to start and end at
the depot and can satisfy a total demand of at most $C$. Each vertex has to be visited by a tour
exactly once.
There are many other variants of the VRP with additional constraints or freedoms, but these are out
of the scope of this thesis.

% Summary of the entire approach.
To solve the TSP and VRP we apply the following stragegy: We start by calculating an initial set of
solutions using heuristics. We then merge all the edges of these solutions into a set of promising
edges $E \subset E'$. After that we merge the solutions into a subgraph $G = (V, E)$. On this graph
we (hopefully) find a tree decomposition with small width $k$. With that decomposition we can
calculate the optimal solution in $G$ using a dynamic programming algorithm that has a running time
exponential in $k$ but linear in the number of vertices. This solution often improves on each of the
solutions of the heuristic (TODO: OR NOT, WAIT FOR RESULTS!).
% Our contribution

% Structure
The paper is organised as follows: in Section~\ref{sec:heuristics} we will discuss the heuristics
used to generate the initial tours and routes. In Section~\ref{sec:td} we will discuss how the
solutions are merged and how the treedecomposition is calculated and in Section~\ref{sec:dp} we will
show the dynamic programming algorithms on the computed decompositions. In Section~\ref{sec:results}
we will discuss the result and finally we conclude in Section~\ref{sec:conclusion}.

% TODO: are we (am I) the only one doing something with the cook-seymour approach? (but one recent: yes)
%   (related work)
% Quote from LKH 1 paper:
%   Symmetric problems are usually more difficult to solve than asymmetric
%   problems [9]. Today the 7397-city problem is the largest (nontrivial) symmetric
%   problem that has been solved. In comparison, the optimal solution of a
%   500,000-city asymmetric problem has been reported [10].
%   [9] M. Bellmore & J. C. Malone,
%   “Pathology of traveling-salesman subtour-elimination algorithms”,
%   Oper. Res., 19, 278-307 (1972).
%   [10] D. L. Miller & J. F. Pekny,
%   “Exact solution of large asymmetric traveling salesman problems”,
%   Science, 251, 754-761 (1991).


%
% Heuristics
%
\section{Heuristics}
\label{sec:heuristics}
Although many different heuristics have been tried to solve the Traveling Salesman Problem, there
are few that can compete with (variants of) the Lin-Kernighan heuristic~\cite{lkh1,
CITE SOMETHING ABOUT THE COMPETING PART HERE}, most notably the implementation of
Helsgaun~\cite{lkh2}.
To find initial solutions for the TSP we chose to use the Lin-Kernighan-Helsgaun heuristic
because it is one of the best heuristics available and its source code is available for academic
use~\cite{lkh-url}. We will discuss the original Lin-Kernighan heuristic in Section~\ref{sec:lk} and
the modifications on the original algorithm by Helsgaun's implementation in Section~\ref{sec:lkh}.

For the Vehicle Routing Problem the currently best heuristics are tabu search algorithms \cite{TODO:
assuming this is even true, the other one}. Unfortunately they often require to finetune a lot of
parameters and are focussed on specific instances of VRP, rather than giving consistent solutions
for all versions \cite{hmmz, where to find a good citation for this one}. Other heuristics like the
classic savings heuristic or the sweep heuristic do give good solutions for all variants of VRP, but
they can't get the results one gets with the tabu heuristics.
For the VRP we chose to use one run of the savings heuristic and multiple runs of the sweep
heuristic, because of their fast running times, reasonable quality of solutions and ease of
implementation. We will discuss these heuristics in Section~\ref{sec:savings} and
Section~\ref{sec:sweep}.
% TODO: With possibly an optimization using LKH - and possibly multiple pertubated runs of savings.

    \subsection{Lin Kernighan}
    \label{sec:lk}
    % TODO: check for the use of 'we' and 'they'
    % Improvement heuristics
    The Lin-Kernighan heuristic~\cite{lin-kernighan} is an improvement heuristic. That means that
    the strategy to solve the TSP consists of the following steps:
    \begin{enumerate}
        \item Generate a (random) initial tour.
        \item Try to find a modification of the tour that improves it.
        \item If an improved solution is found, replace the tour and repeat from step~2.
        \item If no improved solutions can be found anymore, we are at a local optimum. We can
            either start again from step~1 or stop, depending on some stopping criterium (e.g.\ the
            solution is good enough, the pool of initial tours is deplenished, or a time limit is
            reached).
    \end{enumerate}

    % Fixed-k opt
    The interseting part of this strategy is step~2: how do we improve on the current tour. One way
    of doing this is to use a $k$-opt algorithm. In a $k$-opt algorithm we try to find two disjoint
    sets of edges $X$ and $Y$, both containing $k$ edges, such that when we remove the edges in $X$
    from the current tour and replace them with the edges from $Y$ the tour will have a lower cost.
    $k$-opt algorithms are well known and often used heuristics because they improve a tour
    effectively while being easy to implement.
    2-opt and 3-opt improvements were proposed for the first time by Croes~\cite{2-opt} and
    Lin~\cite{3-opt}, respecively in 1958 and 1965. $k$-opts with higher values of $k$ have been
    tried as well, for example by Christofides and Eilon~\cite{2-5-opt}, who tried values for $k$ up
    to 5.
    However, using $k$-opt for fixed $k$ has its limitations. It is unknown beforehand which value
    of $k$ will give a good result for the running time involved, as it costs substantially more
    time to find the edge sets and their improvements for increasing values of $k$.

    % The actual generalized k-opt
    The Lin-Kernighan algorithm is a $k$-opt algorithm, but for a dynamic $k$. The edge sets to be
    replaced are equal to a sequence of 2-opt exchanges, possibly preceeded by a single 3-opt. The
    difference between this approach and repeatedly applying 2-opt optimizations is that not every
    part of the sequence has to improve on the cost of the tour; it is the cost of the entire
    sequence that matters.
    Another difference with the fixed $k$-opt algorithms is in how we choose the sets of edges to be
    removed or inserted. In the 2-opt algorithm we first choose the set $X$ of edges in the original
    tour to be removed (i.e.\ two crossing edges with the euclidean metric) and then find the
    corresponding set $Y$ of new edges to be inserted in the tour. In the Lin-Kernighan algorithm,
    the sets $X$ and $Y$ are built up step by step, as the sequence grows.

    In the algorithm we start by choosing an initial vertex $t_1$ and choose one of the two
    adjacent edges $x_1 = (t_1, t_2)$ in the original tour.
    After we have chosen the first edge $x_1$, we repeatedly choose edges $y_i = (t_{2i}, t_{2i+1})$
    and $x_{i+1} = (t_{2i+1}, t_{2i+2})$, according to some criteria.
    Note that $X$ contains one edge more than $Y$. Because of that, if we remove the edges in $X$
    from the current tour and add the edges in $Y$ to it, the result will not be a tour but a path.
    However, if $X$ and $Y$ are constructed in the right way, we can close the path by adding the
    edge $y_{i+1} = (t_{2i+2}, t_1)$ to get a tour.
    We choose an edge $y_i$ from a set of edges to the 5 nearest neighbours of $t_{2i}$. This edge
    $y_i$ may not be in the original tour already and the current sequence must have a positive
    gain, i.e.\ $\sum_{j=0}^i x_j - y_j > 0$. Furthermore, $y_i$ should be chosen such that a next
    edge $x_{i+1}$ exists (i.e.\ $x_{i+1}$ is not chosen already).
    The edge $x_{i+1}$ is uniquely defined for all $i \geq 1$, as there are only two edges adjacent
    to $t_{2i}$ in a tour, and if the wrong one is chosen the tour cannot be closed anymore (see
    Figure~\ref{fig:unique-x}).
    An exception is made for $x_2$, where the wrong edge choice is allowed as it can be fixed by
    the choice for $y_3$ and $x_4$.
    If there are multiple valid choices for $y_i$ or $x_{i+1}$, we initially choose the one with the
    highest gain. For $y_i$ we look ahead and choose the edge where $x_{i+1} - y_i$ is largest.
    For $x_{i+1}$ we choose the edge with the largest weight. Other choices are ignored at first,
    but may be examined via backtracking.
    We keep adding edges $y_i$ and $x_{i+1}$ to the sets, untill either the complete sequence
    (including the last edge $y_{i+1}$ that will close the tour) is shorter than the previous tour
    (in which case we succeeded) or that there is no edge $y_i$ to find that improves the tour even
    if the closing edge $y_{i+1}$ would have weight 0 (in which case we failed).
    If we fail, we can either stop, start again from scratch (with a different vertex for $t_1$), or
    use backtracking. The latter means that we go back a few steps and try another edge $y_i$ (or
    occasionally another edge $x_{i+1}$).
    As backtracking is quite time consuming, we only allow it at the first two levels ($i \leq 2$).
    An example of the dynamic $k$-opt procedure is shown in Figure~\ref{fig:lk-example}.

    This describes the overview of the Lin-Kernighan heuristic. We will describe two more
    optimizations below, one to speed up the algorithm, the other to improve the solution. There are
    many more details which are essential for the efficiency of the algorithm however, as it is not
    a simple algorithm to implement~\cite{lkh1}. We omit them here as they are not essential to this
    thesis. For the exact details we refer to the original paper~\cite{lin-kernighan}.

    The first optimization reduces the choices for the edges. In the first few solutions found by
    the algorithm there always are a lot of edges that appear in every solution. These common edges
    are recorded and then they are no longer allowed to be broken for the other solutions. This
    means they cannot be used as choice for $x_{i+1}$ any more, which in turn limits the choices for
    $y_i$ and speeds up the overall algorithm significantly. Note that in order to not bias the
    solution too much, we only use this restriction for $i \geq 4$.

    In the second optimization we apply the double bridge move.
    The dynamic $k$-opt procedure that we described here only allows us to find so called sequential
    moves: a series of connected edges, alternating inside and outside the original tour. Not all
    possible $k$-opts consist of such a sequence however, and not considering these moves can
    sometimes be the difference between a good solution and the optimal solution. The easiest
    example of a non sequential move is a 4-opt known as the the double-bridge move (see
    Figure~\ref{fig:double-bridge}).
    It is tried as a post-optimization after we finished with the entire algorithm and only for
    edges that are not amoungst the common edges from the previous optimization. Lin and Kernighan
    found that in some cases this improved the result significantly, while in other cases it did
    nothing. It doesn't hurt to try though, as it is a relatively cheap optimization.

    % Nice summary in LKH 2 paper: 1-5 on page 5 of 45 (p. 123)
    % 1: The sequential exchange cirterion
    % 2: The feasibility criterion
    % 3: The positive gain criterion
    % 4: The disjunctivity criterion
    % 5: The candidate set criterion

    \begin{figure}
    \centering
    \begin{tabular}{c c}
        \begin{tikzpicture}[auto,swap]
        \node[vertex1] (0) at (1.2, -2.9) {};
        \node[vertex1] (1) at (1.4, 0.1) {};
        \node[vertex1] (2) at (2.0, 0.3) {};
        \node[vertex0] (3) at (4.1, -1.2) {};
        \node[vertex0] (4) at (3.9, -2.4) {};
        \node[vertex1] (5) at (1.7, -3.2) {};
        \path[edge,bend left=48] (0) to (1);
        \path[edge] (0) to node[below] {$x_1$} (5);
        \path[edge] (0) to node[right] {$y_1$} (2);
        \path[edge] (1) to node[above] {$x_2$} (2);
        \path[edge,dashed] (1) to node[left] {$y_2$} (5);
        \path[edge,bend left=36] (2) to (3);
        \path[edge,bend left=19] (3) to (4);
        \path[edge,bend left=32] (4) to (5);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[auto,swap]
        \node[vertex1] (0) at (1.2, -2.9) {};
        \node[vertex1] (1) at (1.4, 0.1) {};
        \node[vertex1] (2) at (2.0, 0.3) {};
        \node[vertex1] (3) at (4.2, -1.2) {};
        \node[vertex1] (4) at (4.2, -1.8) {};
        \node[vertex1] (5) at (1.7, -3.2) {};
        \path[edge,bend left=48] (0) to (1);
        \path[edge] (0) to node[below] {$x_1$} (5);
        \path[edge] (0) to node[right] {$y_1$} (2);
        \path[edge] (1) to node[above] {$x_2$} (2);
        \path[edge] (1) to node[left] {$y_2$} (4);
        \path[edge,bend left=36] (2) to (3);
        \path[edge] (3) to node[right] {$x_3$} (4);
        \path[edge,dashed] (3) to node[left] {$y_3$} (5);
        \path[edge,bend left=42] (4) to (5);
        \end{tikzpicture}
        \\
    \end{tabular}
    \caption{An example of a 3-opt move, as constructed by the Lin-Kernighan heuristic. Note that
    vertices are displayed in a circle in the order they appear in the original tour.}
    \label{fig:lk-example}
    \end{figure}

    \begin{figure}
    \centering
    \begin{tikzpicture}[auto,swap]
    \node[vertex1] (0) at (1.2, -2.9) {};
    \node[vertex1] (1) at (1.4, 0.1) {};
    \node[vertex1] (2) at (2.0, 0.3) {};
    \node[vertex1] (3) at (4.2, -1.8) {};
    \node[vertex1] (4) at (4.0, -2.4) {};
    \node[vertex1] (5) at (1.7, -3.2) {};
    \path[edge,bend left=48] (0) to (1);
    \path[edge] (0) to node[below] {$x_1$} (5);
    \path[edge] (0) to node[right] {$y_1$} (2);
    \path[edge] (1) to node[above] {$x_2$} (2);
    \path[edge] (1) to node[left] {$y_2$} (3);
    \path[edge,bend left=43] (2) to (3);
    \path[edge] (3) to node[right] {$x_3$} (4);
    \path[edge,dashed] (4) to node[left] {$y_3$} (5);
    \path[edge,bend left=32] (4) to (5);
    \end{tikzpicture}
    \caption{An example where the wrong choice for $x_3$ is made, resulting in two disjunct tours.}
    \label{fig:unique-x}
    \end{figure}
    % TODO: wrong choice for x_2 ??? That's where I refer to it...

    \begin{figure}
    \centering
    \begin{tikzpicture}[auto,swap]
    \node[vertex1] (0) at (7.0, -0.94) {};
    \node[vertex1] (1) at (7.7, -0.94) {};
    \node[vertex1] (2) at (7.0, -4.5) {};
    \node[vertex1] (3) at (7.7, -4.5) {};
    \node[vertex1] (4) at (9.1, -2.4) {};
    \node[vertex1] (5) at (9.1, -3.1) {};
    \node[vertex1] (6) at (5.5, -2.4) {};
    \node[vertex1] (7) at (5.5, -3.1) {};
    \path[edge] (0) to node[left] {$y_1$} (2);
    \path[edge,bend right=30] (0) to (6);
    \path[edge] (0) to node[above] {$x_2$} (1);
    \path[edge,bend left=30] (1) to (4);
    \path[edge] (1) to node[right] {$y_2$} (3);
    \path[edge] (2) to node[below] {$x_1$} (3);
    \path[edge,bend left=30] (2) to (7);
    \path[edge,bend right=30] (3) to (5);
    \path[edge] (4) to node[right] {$x_4$} (5);
    \path[edge] (4) to node[above] {$y_3$} (6);
    \path[edge] (5) to node[below] {$y_4$} (7);
    \path[edge] (6) to node[left] {$x_3$} (7);
    \end{tikzpicture}
    \caption{The double bridge move}
    \label{fig:double-bridge}
    \end{figure}


    \subsection{Lin Kernighan Helsgaun}
    \label{sec:lkh}
    The Lin-Kernighan-Helsgaun algorithm~\cite{lkh1, lkh2} is based on the algorithm of Lin and
    Kernighan, but improves on several points.

    % Minimum 1 tree instead of 5 nearest neighbours (section 4.1 (and 4.2 for x_1) in LKH 1 paper)
    The first major difference with the Lin-Kernighan algorithm is the candidate set for the edges
    in $Y$. In the original algorithm the choices for an edge $y_i$ are limited to the edges to the
    first 5 nearest neighbours. The choice for this candidate set assumes that the shorter an edge
    is, the higher the chance is that it occurs in a tour. This is reasonable, but it is not always
    a good estimation. Helsgaun notices that the distance to a minimum 1-tree, called
    \emph{$\alpha$-nearness}, is a better estimation.
    A 1-tree for a graph $G=(V, E)$ is a spanning tree on the vertex set $V \setminus \{1\}$
    combined with two edges from $E$ incident to the 1-vertex. An example of a 1-tree is shown in
    Figure~\ref{fig:1-tree}. Note that a 1-tree is not a tree (as it contains a cycle) and that the
    choice of the 1-vertex is arbitrary. A minimum 1-tree is a 1-tree of minimum length. The
    $\alpha$-nearness of an edge $e$ is defined as the difference between the length of the smallest
    1-tree containing $e$ and the length of the minimum 1-tree. Or in other words: $\alpha$-nearness
    is the increase in length of the minimum 1-tree if it is required to contain the edge $e$.
    % TODO: subgradient optimization (change the min 1-tree a bit, see bottem of page 23 and below of LKH 1 paper)
    The candidate set then is further modified to always include edges that are in both of the two
    previous best solutions. These edges are tried first.
    The candidate set for the first edge to be chosen, $x_1$, is also changed so that no edges from
    the previous best tour are removed in the first level of the improvement step.

    \begin{figure}
    \centering
    \begin{tikzpicture}[auto,swap]
    \node[vertex1] (0) at (5.1, -0.89) {};
    \node[vertex1] (1) at (4.1, -1.8) {};
    \node[vertex1] (2) at (2.9, -2.8) {};
    \node[vertex1] (3) at (4.9, -2.8) {};
    \node[vertex1] (4) at (4.9, -4.2) {};
    \node[vertex1] (5) at (6.1, -2.0) {};
    \node[vertex1] (6) at (7.1, -2.9) {};
    \node[vertex1] (7) at (8.2, -2.0) {};
    \node[vertex1] (8) at (8.3, -3.6) {};
    \node[vertex1] (9) at (7.1, -4.4) {};
    \node[vertex1,label=0:\;The 1-vertex] (10) at (5.9, -5.3) {};
    \path[edge] (0) to (1);
    \path[edge] (0) to (5);
    \path[edge] (1) to (2);
    \path[edge] (1) to (3);
    \path[edge] (3) to (4);
    \path[edge] (4) to (10);
    \path[edge] (5) to (6);
    \path[edge] (6) to (7);
    \path[edge] (6) to (8);
    \path[edge] (6) to (9);
    \path[edge] (9) to (10);
    \end{tikzpicture}
    \caption{A 1-tree}
    \label{fig:1-tree}
    \end{figure}

    The second major change is the choice of the basic move. In the original Lin-Kernighan algorithm
    every move was composed as a sequence of 2-opt moves (and possibly a 3-opt).
    Helsgaun~\cite{lkh1} modified it to use moves that consist of 5-opt moves (unless a $k$-opt move
    for smaller $k$ results in an improvement already). Later he modified it again to use general
    $k$-opt moves up to a certain $k$ that we can choose ourselves as the basic step. The steps to
    do this are rather involved and contain a large case-analysis and won't be described here in
    detail. For the details we refer to the paper of Helsgaun~\cite{lkh2}.
    The main idea of his approach however is that we allow edges $x_i$ that initially break up the
    tour (like in Figure~\ref{fig:unique-x}). We then look ahead to the following edges $y_{i+1}$
    and $x_{i+1}$ to make sure that acceptable edges exist that can fix the tour. These edges in
    their turn do not have to be chosen; we can choose edges with higher gain that break up the tour
    again, providing we look ahead and find acceptable edges that can fix the tour. This allows,
    amoung others, the double bridge move from Figure~\ref{fig:double-bridge} to be included
    natively in the search.
    % TODO: double bridge move
    % - Basic move no longer 2- (or 3-) opt move, but a 5-opt move (but stopped as soon as it
    %   improves, so it can also do just a 2-opt move for example. Also backtracking removed.
    %   5-opt as basic step adds quite a lot, as observed by \cite{2-5-opt}.
    %   Also something with non-sequential moves (no longer just post-optimization). TODO: note in sec:lk.
    % - General k-opt moves

    % Initial tours
    The third change is not aimed at improving the quality of the solutions, but aims to speed up
    the algorithm.
    As in most improvement heuristics, the Lin-Kernighan algorithm is tried several times on
    different initial tours. These tours where constructed randomly because Lin and Kernighan
    considered construction heuristics to be unnescessary. Helsgaun notes that even though a good
    construction heuristic does not significantly improve the quality of the final tour, it does
    improve the running time of the algorithm.
    The heuristic used by Helsgaun tries to construct a tour greedily by choosing edges that are in
    the minimum 1-tree and that are in the previous best tour. If no such edge is found either a
    random other candidate edge is chosen or, if they are not valid either, an edge to any free
    vertex.

    % Other points from LKH 2 paper:
    Amoungst some more additions added by Helsgaun is the option of merging tours.
    Similar to what we do in this thesis he merges the edges of a few solutions in a single graph
    and on this merged graph he solves the TSP again.
    Unlike what we do in this thesis he doesn't solve the merged problem to optimality, but applies
    the general $k$-opt submoves again. This time he uses larger value of $k$, which he can do
    because the graph is sparse. We disable this option in our experiments, as we perform this
    calculation ourselves.
    There are some more additions, but Helsgaun doesn't go into much detail on them, so for these we
    refer to Helsgaun's paper~\cite{lkh2}.

    % % Original text of me
    % % Partitioning
    % The Lin-Kernighan-Helsgaun algorithm also adds the possibility of partitioning the original
    % graph into multiple subsets that are solved seperately and then used to improve a given overall
    % tour. Six schemes are allowed: Tour segment, Karp, Delaunay, Rohe, K-means and Space-filling
    % Curve partitioning~\cite{TODO}.

    % % Tour merging
    % The heuristic also tries to merge tours. Unlike what we do in this thesis he doesn't solve the
    % merged problem to optimality, but applies the general $k$-opt submoves again, but this time with
    % a larger value of $k$, because the graph is sparse. For this thesis this option is disabled.

    % % Iterative partial transcription
    % Another technique used to inmprove the tour is Iterative partial transcription. This attempts to
    % find subchains of different solutions with the same endpoints and replace them with the shorter
    % one. The algorithm used to find these chains is a simplified version of the algorithm by Mobius
    % et al.~\cite{TODO - 29 of LKH2 paper}.

    % % Backbone-guided search
    % Lastly the Lin-Kernighan-Helsgaun algorithm implements a simplified version of the algorithm by
    % Zhang and Looks~\cite{TODO - 34 of LKH2 paper} for a backbone-guided search. This means that
    % edges of a fixed number of initial tours are used as candidate edges in later tours.



%
% Merging and Tree Decomposition calculations
%
\section{Tree decomposition}
\label{sec:td}
% Merging
Once we have generated a set of good tours for our original graph $G'=(V,E')$, we merge these
tours in one graph. All tours $E'_j \subset E'$, for $0 \leq j < \#tours$ as found by the
heuristics are merged together into a new graph $G = (V, E)$, where $E=\bigcup E'_j$. For this graph
$G$ we will compute a tree decomposition so that we can compute the optimal tour in this reduced
problem.
% Roadmap:
Before we show how we compute this decomposition in Section~\ref{sec:td-heuristic}, we first give
the defenition of a tree decomposition in Section!\ref{sec:td-definition}.

    % What is a tree decomposition
    \subsection{Tree decomposition and width}
    \label{sec:td-definition}
    A \emph{tree decomposition} of a graph $G=(V, E)$ is a pair $(T=(W, H), X)$, where $T$ is a tree
    with an arbitrary root vertex and $X=\{X_i \subset V : i \in W\}$ a set of \emph{bags},
    satisfying:
    \begin{enumerate}
        \item $\bigcup_{i \in W} X_i = V,$
        \item for all $(u, v) \in E$ there is an $i \in W$ with $u, v \in X_i$ and
        \item for all $v \in V$, the set $W_v = \{i \in W: v \in X_i\}$ forms a connected subtree of $T$.
    \end{enumerate}
    The \emph{width} $k$ of the tree decomposition is $\max_{i \in W} |X_i| - 1$. The \emph{treewidth}
    of a graph $G$, is the minimum width among all tree decompositions of $G$.
    An example of a tree decomposition is shown in Figure~\ref{fig:td}.

    \begin{figure}
    \centering
    \begin{tikzpicture}[auto,swap]
    \node[vertex2] (0) at (0.59, -3.7) {0};
    \node[vertex2] (1) at (2.3, -0.78) {1};
    \node[vertex2] (2) at (3.0, -2.5) {2};
    \node[vertex2] (3) at (5.2, -2.4) {3};
    \node[vertex2] (4) at (3.0, -4.2) {4};
    \node[vertex2] (5) at (4.7, -5.6) {5};
    \node[vertex2] (6) at (1.6, -6.0) {6};
    \path[edge] (0) to (1);
    \path[edge] (0) to (4);
    \path[edge] (0) to (6);
    \path[edge] (1) to (2);
    \path[edge] (1) to (3);
    \path[edge] (2) to (3);
    \path[edge] (2) to (4);
    \path[edge] (3) to (4);
    \path[edge] (3) to (5);
    \path[edge] (4) to (5);
    \path[edge] (5) to (6);
    \node[bag] (0) at (9.5, -1.2) {0: 0, 1, 3, 4};
    \node[bag] (1) at (8.0, -3.3) {1: 1, 2, 3, 4};
    \node[bag] (2) at (1.1e+01, -3.3) {2: 0, 3, 4, 5};
    \node[bag] (3) at (1.1e+01, -5.8) {3: 0, 5, 6};
    \path[edge] (0) to (1);
    \path[edge] (0) to (2);
    \path[edge] (2) to (3);
    \end{tikzpicture}
    \caption{An example of a graph $G$ and its tree decomposition.}
    \label{fig:td}
    \end{figure}

    Throughout this thesis we often work with the edge set corresponding to the vertex $i \in W$, rather
    than the vertex set $X_i$ itself. To that end we define $Y_i = \{(u,v) \in E: u,v \in X_i \}$.
    We say that a bag contains a vertex $v$ if $v \in X_i$ and that it contains an edge $e$ if $e \in Y_i$.


    % Minimum Degree Heuristic
    \subsection{Minimum Degree Heuristic}
    \label{sec:td-heuristic}
    Calculating the optimal treewidth or the optimal tree decomposition is an \ccNP-Hard problem
    \cite{find ref}, so finding an optimal decomposition in reasonable time is infeasable unless
    \ccP=\ccNP\@. We do not nescessarily need a tree decomposition of optimal width, we just need the
    width to be sufficiently small so that our DP algorithm runs fast enough. Therefore, we compute
    our tree decomposition with a heuristic.

    Bodlaender and Koster~\cite{tw-upper-bounds} evaluated a number of construction heuristics. We
    chose to use the Minimum Degree Heuristic, originally designed by Markowitz~\cite{min-degree},
    because it is a simple but effective heuristic. It is fast, obtains results close to the optimum
    and is easy to implement.
    A non-recursive version of the algorithm consists of the following steps:
    \begin{enumerate}
        \itemsep 0em
        \item
            Initially let $(T=(W, H), X)$ with $W$, $H$ and $X$ set to $\emptyset$.
        \item
        \label{enum:min-degree-begin}
            Take the vertex $v \in V$ with minimum degree and add it to $W$; i.e.\ add a new
            vertex to $W$ with the same name as $v$.
            The reason to give it the same name is that it allows us to add edges ahead of time in
            step~\ref{enum:min-degree-add-edge}.
        \item
            Create a bag $X_v$ with $v$ and all its neighbours in $G$.
        \item
        \label{enum:min-degree-add-edge}
            Add an edge $(v, w)$ to $H$, where $w$ is the neighbour of $v$ in $G$ with the smallest
            degree. Note that $w$ is not yet added to $W$, but will be added in the future.
        \item
        \label{enum:min-degree-end}
            Modify $G$ by turning all the neighbours of $v$ into a clique and removing $v$ from
            $V$ (and its incident edges from $E$).
        \item
            Repeat step~\ref{enum:min-degree-begin} to~\ref{enum:min-degree-end} untill all vertices
            are processed (i.e.\ $V = \emptyset$).
    \end{enumerate}
    To complete the tree decomposition we choose the first vertex of $W$ to be the root of the tree.

    As a small optimization step specific to the TSP and VRP we remove the last two vertices from
    $W$. The corresponding bags only contain 1 or 2 vertices and are subsets of another bag. We can
    do this because the graph $G$ is obtained by merging a set of tours, and therefore every vertex
    is guaranteed to have at least two neighbours.
    When we are solving the VRP, we also add the depot vertex to every bag.

    % TODO: the depot vertex causes G to be not 2-connected! - Make sure this doesn't cause any trouble!

    % TODO: trick of that other students paper to not merge all graphs?



%
% The dynamic programming algorithms
%
\section{Dynamic programming}
\label{sec:dp}
Provided the width is small enough, the optimal solution for the TSP or VRP on the merged graph can
be computed using a dynamic programming algorithm on the tree decomposition of the graph. In the
following sections we explain the details of the algorithms.
% TODO: roadmap

    \subsection{Traveling Salesman}
    \label{sec:dp-tsp}
    % 2-connectedness: to make the graph disjoint, you need to remove at least 2 vertices
    % Simle graph: G is unweighted (?); undirected; no double edges; no self loops.
    Let $G=(V, E)$ be a simple graph with edge-weights $c_e$ and $(T=(W, H), X)$ be the tree
    decomposition with width $k - 1$ and $X_i$ and $Y_i$ be the bags with respecitvely vertices or
    edges as defined in Section~\ref{sec:td-definition}. We say that a bag $X_j$ is below a bag
    $X_i$ (in the tree) if $i$ is on the path from $j$ to the root of $T$.
    Note that because $G$ is the result of a number of merged tours, it is 2-connected and all
    vertices have a degree of at least two.
    The main idea of the algorithm is to find a series of disjoint paths and connect them together
    into a Hamiltonian tour of minimum weight. A series of these paths start and end in a bag, and
    visit all vertices in bags below that bag in the tree. Such a series of paths is encoded using
    vertex degrees and a matching.  Every vertex can have degree 0, 1 or 2. Vertices with degree 2
    are already \emph{used} in a path, vertices with degree 1 are \emph{endpoints} of a path and
    vertices with degree 0 are \emph{free}, so not yet used in any of the paths. For every pair of
    endpoints we have an edge $\{u, v\}: u,v \in V$ in the \emph{matching} to mark which vertices
    are the endpoints of a path.

    % TODO: what is the degree of an edge set
    We now define the function $F(X_i, D, M)$ to be the minimum total cost of the edges in a series
    of paths starting and ending in bag $X_i$, where $D$ is a set of degrees for the vertices in
    $X_i$ and $M$ a matching. If there is an edge $\{u, v\} \in M$ then there should be a path that
    starts in $u$ and ends in $v$. All vertices in $X_i$ itself should have degrees as given in the
    degrees parameter, and the vertices that occur only in the bags below $X_i$ in the tree should
    all be used (have a degree of 2).

    One way of looking at this is to see the set of degrees $D$ as an instruction to a specific part
    of the tree (the bag $X_i$ and all bags below in the tree) to deliver a set of edges, together
    forming a series of disjoint paths, such that all the degrees of vertices in this bag match with
    the degrees in $D$ and that all vertices that occur only in bags below $X_i$ in the tree are
    used.
    Of course, we do not just want any set of edges, we want the edges that can do it with the
    minimum cost. To get the cost of a tour through the entire graph, we can now call
    $F(X_0, D_0=\{(v, 2),$ for $v \in X_0\}, \emptyset)$. The root of the tree is the special case
    where we allow the paths to form a (single) cycle. Therefore if we give the instruction to the
    root bag $X_0$ to give us a set of edges such that all the vertices inside $X_0$ itself have
    degree 2 (as required by the set $D_0$) and all vertices in bags below the root have degree 2 as
    well (by specification of the function $F$), we actually give the instruction to find the weight
    ofa set of edges that visits all the vertices of $G$ in a single cycle. And because this set of
    edges should have minimum cost, this gives us the cost of the TSP tour through $G$.

    Of course, for a good tree decomposition not all the edges are contained in a single bag. The
    main problem for a non-leaf bag $X_i$ now is not how to find a subset of edges in $Y_i$ that
    satisfy all the requirements for the degrees (and the matching), but how to divide the degrees
    over it's children so that they (recursively) can find the right edge sets that satisfy their
    part of the requirements. Selecting some edges from $Y_i$ is mostly used to stitch the different
    paths from the child bags together so that the complete series of paths meets the requirements
    of $D$ and $M$.
    % TODO: picture

    To find the different ways of dividing the requirements for a bag over it's children, we focus
    solely on the degrees. We also have to decide on the edges in the matching of the children, but
    we will find them as we set the requirements on the degrees.
    We try all possible combinations of dividing the degrees per vertex. For a given vertex $v$,
    assuming we are required to give it a degree of 2, we first try to give it to each of the
    children. So for possibility $p$ and child bag $j$ we try to set the degree of $v$ in $D_{p,j}$
    to 2. Afterwards, assuming we have to give $v$ a degree of at least 1, we try to use it as an
    endpoint coupled with each of the vertices in all of the child bags. So the degrees of two
    vertices, $v$ and some other vertex $u$ in $D_{p,j}$ are set to 1. We try this for all (valid)
    combinations of $u$ and $j$. At this step we also add the edge $\{u, v\}$ to $M_{p,j}$. Finally
    we try not to assign $v$ to any of the child bags, so that we can give its degree with one of
    $Y_i$'s edges.
    Of course, vertices are only given to a child bag if it contains the vertex.

    For the remaining degrees that are not handled by any of the child bags we calculate a subset
    $E_p$ of $Y_i$. For non-leaf bags these edges mainly glue paths from the children together in
    the paths as required by the $D$ and $M$ parameters. For the leaf bags there are of course no child
    bags to delegate the degrees to so it all has to be solved using the bags own edges.
    Note that the edge set is not allowed to introduce cycles, so in particular two endpoints in an
    edge of a matching are not allowed to be connected. This is the reason why we need to give the
    matching as parameter to $F$, without it we do not have sufficient information to determine
    which vertices can and which vertices cannot be connected. The root bag is of course an
    exception, because there all paths are merged in a single cycle.

    In summary, a vertex' degree can be satisfied by passing it on to one (or two) of the child bags
    or in the bag itself by choosing an edge from $Y_i$. Formally this becomes
    \[
        F(X_i, D, M) = \min_{1 \leq p \leq P_i} (
            \sum_{j \in W :\text{\ Parent}(j) = i} F(X_j, D_{p,j}, M_{p,j}) + \sum_{e \in E_p} c_e
        )
    \]
    for all $P_i$ ways of dividing $D$ and $M$ into the $D_{p,j}$ and $M_{p,j}$ sets and the
    corresponding $E_p \subset Y_i$.
    If no valid edge set is found, $F(X_i, D, M) = \infty$.

    The overall algorithm then consists of a top down approach where we tabulate all entries for
    the function $F$, starting at the root and then recursively work downwards in the tree. Then the
    value of each table entry is finished bottem up as the recursion returns the values for the
    child entries.

    % Overview:
    % - Some defenitions
    % - Idea: Disjoint paths
    % - Vertex degrees: 0, 1, 2
    % - Matchings for the 1's
    % - Talk about subproblems / partial tours / table-entries / functions
    % - The root bag
    % - Top down approach; recursive
    % - It tabulates the degrees+matchings


    \subsection{Vehicle Routing}
    \label{sec:dp-vrp}
    Let $G=(V, E)$ again be a simple graph with edge-weights $c_e$ and $(T=(W, H), X)$ be the tree
    decomposition with width $k - 1$ and $X_i$ and $Y_i$ be the bags with respecitvely vertices or
    edges as defined in Section~\ref{sec:td-definition}. We say that a bag $X_j$ is below a bag
    $X_i$ (in the tree) if $i$ is on the path from $j$ to the root of $T$.
    Furthermore let $M$ be the number of trucks that we have to use (and therefore te number of
    tours that we should find) and let $C$ be the capacity of each truck. We denote the demand of
    all vertices $v_i \in V$ by $d_i$, and the demand $d_0$ of the depot vertex $v_0$ is equal to 0.
    Note that $G$ is not 2-connected as was the case for the TSP, as removing the depot vertex
    causes it to be disconnected, but it still holds that all vertices have a degree of two or more.

    The main idea of the algorithm is the same as the algorithm for the TSP\@. We try to find a
    series paths that together form set of tours. The instruction of how these paths should be
    delivered by a bag is again done by specifying the degrees of the vertices and a matching. The
    degrees of all non-depot vertices can be one of $\{0, 1, 2\}$ like before, but the degree of the
    depot vertex can be any value between 0 and $2M$. Furthermore we have a restriction that the
    depot can only appear as an endpoint of a path. The matching-edges used to encode a path are
    extended with an edge-weight $c_{u, v}$, which denotes that the total demand of vertices on the
    path from $u$ to $v$ can be at most $c_{u, v}$. While they look simple, these changes complicate
    the algorithm quite a lot.

    TODO:
    - Instead of considering child bags, consider all paths in all childs.
    - Try all different demand combinations on these paths.
    - Changed edge selection.
    - Merge paths in root bag.
    - Extra requirements (depot vertex only as endpoints, \dots).


    \subsection{Speed}
    Although DP running time upperbounds of $O(n 3^k 2^{k^2}) - TODO$??? and $O(M n 3^k 2^{\dots})$
    are terrible, in practice these limits are never reached. This is because edges\dots TODO

    % Variations:
    % - Why this variation? (because easy)
    % - different objective value. minimize max cost per tour (add M-depot vertices, and connect
    %   only equals.



%
% Results
%
\section{Results}
\label{sec:results}
Todo



%
% Conclusion
%
\section{Conclusion}
\label{sec:conclusion}
Todo



%
% Bibliography
%
\begin{thebibliography}{9}
    % The most important references
    \bibitem{cook-seymour}
        Cook, W., \& Seymour, P. (2003).
        \emph{Tour merging via branch-decomposition.}
        INFORMS Journal on Computing, 15(3), 233-248.

    \bibitem{2-opt}
        Croes, G. A. (1958).
        \emph{A method for solving traveling-salesman problems.}
        Operations research, 6(6), 791-812.

    \bibitem{3-opt}
        Lin, S. (1965).
        \emph{Computer solutions of the traveling salesman problem.}
        Bell System Technical Journal, The, 44(10), 2245-2269.

    \bibitem{2-5-opt}
        Christofides, N., \& Eilon, S. (1972).
        \emph{Algorithms for large-scale travelling salesman problems.}
        Operational Research Quarterly, 511-518.

    \bibitem{lin-kernighan}
        Lin, S., \& Kernighan, B. W. (1973).
        \emph{An effective heuristic algorithm for the traveling-salesman problem.}
        Operations research, 21(2), 498-516.

    \bibitem{lkh1}
        Helsgaun, K. (2000).
        \emph{An effective implementation of the Lin–Kernighan traveling salesman heuristic.}
        European Journal of Operational Research, 126(1), 106-130.

    \bibitem{lkh2}
        Helsgaun, K. (2009).
        \emph{General k-opt submoves for the Lin–Kernighan TSP heuristic.}
        Mathematical Programming Computation, 1(2-3), 119-163.

    \bibitem{lkh-url}
        Helsgaun, K.
        \emph{LKH}
        \url{http://www.akira.ruc.dk/~keld/research/LKH/}

    \bibitem{tw-upper-bounds}
        Bodlaender, H. L., \& Koster, A. M. (2010).
        \emph{Treewidth computations I. Upper bounds.}
        Information and Computation, 208(3), 259-275.

    \bibitem{min-degree}
        Markowitz, H. M. (1957).
        \emph{The elimination form of the inverse and its application to linear programming.}
        Management Science, 3(3), 255-269.

\end{thebibliography}

\end{document}
